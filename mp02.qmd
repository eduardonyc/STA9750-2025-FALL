---
title: "STA 9750 Mini-Project #02: Making Backyards Affordable for All"
author: "Eduardo Alarcon"
format: 
  html:
    code-fold: true
    code-summary: "Show code"
execute:
  message: false
  warning: false
---

<style>
strong, b {
  font-weight: bold !important;
}
</style>


```{r}
#| label: setup
#| message: false
#| warning: false

# Load all required packages
library(tidyverse)   # includes dplyr, purrr, ggplot2, readr, etc.
library(glue)
library(readxl)
library(tidycensus)
library(httr2)
library(rvest)
library(DiagrammeR)
library(kableExtra)
library(stringr)
library(gt)

```

## Data Acquisition

### Task 1: Data Import
```{r}
if(!dir.exists(file.path("data", "mp02"))){
    dir.create(file.path("data", "mp02"), showWarnings=FALSE, recursive=TRUE)
}

library <- function(pkg){
    ## Mask base::library() to automatically install packages if needed
    ## Masking is important here so downlit picks up packages and links
    ## to documentation
    pkg <- as.character(substitute(pkg))
    options(repos = c(CRAN = "https://cloud.r-project.org"))
    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)
    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))
}

library(tidyverse)
library(glue)
library(readxl)
library(tidycensus)

get_acs_all_years <- function(variable, geography="cbsa",
                              start_year=2009, end_year=2023){
    fname <- glue("{variable}_{geography}_{start_year}_{end_year}.csv")
    fname <- file.path("data", "mp02", fname)
    
    if(!file.exists(fname)){
        YEARS <- seq(start_year, end_year)
        YEARS <- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)
        
        ALL_DATA <- map(YEARS, function(yy){
            tidycensus::get_acs(geography, variable, year=yy, survey="acs1") |>
                mutate(year=yy) |>
                select(-moe, -variable) |>
                rename(!!variable := estimate)
        }) |> bind_rows()
        
        write_csv(ALL_DATA, fname)
    }
    
    read_csv(fname, show_col_types=FALSE)
}

# Household income (12 month)
INCOME <- get_acs_all_years("B19013_001") |>
    rename(household_income = B19013_001)

# Monthly rent
RENT <- get_acs_all_years("B25064_001") |>
    rename(monthly_rent = B25064_001)

# Total population
POPULATION <- get_acs_all_years("B01003_001") |>
    rename(population = B01003_001)

# Total number of households
HOUSEHOLDS <- get_acs_all_years("B11001_001") |>
    rename(households = B11001_001)
```


```{r}
get_building_permits <- function(start_year = 2009, end_year = 2023){
    fname <- glue("housing_units_{start_year}_{end_year}.csv")
    fname <- file.path("data", "mp02", fname)
    
    if(!file.exists(fname)){
        HISTORICAL_YEARS <- seq(start_year, 2018)
        
        HISTORICAL_DATA <- map(HISTORICAL_YEARS, function(yy){
            historical_url <- glue("https://www.census.gov/construction/bps/txt/tb3u{yy}.txt")
                
            LINES <- readLines(historical_url)[-c(1:11)]

            CBSA_LINES <- str_detect(LINES, "^[[:digit:]]")
            CBSA <- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))

            PERMIT_LINES <- str_detect(str_sub(LINES, 48, 53), "[[:digit:]]")
            PERMITS <- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))
            
            data_frame(CBSA = CBSA,
                       new_housing_units_permitted = PERMITS, 
                       year = yy)
        }) |> bind_rows()
        
        CURRENT_YEARS <- seq(2019, end_year)
        
        CURRENT_DATA <- map(CURRENT_YEARS, function(yy){
            current_url <- glue("https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls")
            
            temp <- tempfile()
            
            download.file(current_url, destfile = temp, mode="wb")
            
            fallback <- function(.f1, .f2){
                function(...){
                    tryCatch(.f1(...), 
                             error=function(e) .f2(...))
                }
            }
            
            reader <- fallback(read_xlsx, read_xls)
            
            reader(temp, skip=5) |>
                na.omit() |>
                select(CBSA, Total) |>
                mutate(year = yy) |>
                rename(new_housing_units_permitted = Total)
        }) |> bind_rows()
        
        ALL_DATA <- rbind(HISTORICAL_DATA, CURRENT_DATA)
        
        write_csv(ALL_DATA, fname)
        
    }
    
    read_csv(fname, show_col_types=FALSE)
}

PERMITS <- get_building_permits()
```


```{r}
library(httr2)
library(rvest)
get_bls_industry_codes <- function(){
    fname <- fname <- file.path("data", "mp02", "bls_industry_codes.csv")
    
    if(!file.exists(fname)){
    
        resp <- request("https://www.bls.gov") |> 
            req_url_path("cew", "classifications", "industry", "industry-titles.htm") |>
            req_headers(`User-Agent` = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0") |> 
            req_error(is_error = \(resp) FALSE) |>
            req_perform()
        
        resp_check_status(resp)
        
        naics_table <- resp_body_html(resp) |>
            html_element("#naics_titles") |> 
            html_table() |>
            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), "NAICS"))) |>
            select(-`Industry Title`) |>
            mutate(depth = if_else(nchar(Code) <= 5, nchar(Code) - 1, NA)) |>
            filter(!is.na(depth))
        
        naics_table <- naics_table |> 
            filter(depth == 4) |> 
            rename(level4_title=title) |> 
            mutate(level1_code = str_sub(Code, end=2), 
                   level2_code = str_sub(Code, end=3), 
                   level3_code = str_sub(Code, end=4)) |>
            left_join(naics_table, join_by(level1_code == Code)) |>
            rename(level1_title=title) |>
            left_join(naics_table, join_by(level2_code == Code)) |>
            rename(level2_title=title) |>
            left_join(naics_table, join_by(level3_code == Code)) |>
            rename(level3_title=title) |>
            select(-starts_with("depth")) |>
            rename(level4_code = Code) |>
            select(level1_title, level2_title, level3_title, level4_title, 
                   level1_code,  level2_code,  level3_code,  level4_code)
    
        write_csv(naics_table, fname)
    }
    
    read_csv(fname, show_col_types=FALSE)
    
}

INDUSTRY_CODES <- get_bls_industry_codes()
```


```{r}
library(httr2)
library(rvest)
library(glue)
get_bls_qcew_annual_averages <- function(start_year=2009, end_year=2023){
    fname <- glue("bls_qcew_{start_year}_{end_year}.csv.gz")
    fname <- file.path("data", "mp02", fname)
    
    YEARS <- seq(start_year, end_year)
    YEARS <- YEARS[YEARS != 2020] # Drop Covid year to match ACS
    
    if(!file.exists(fname)){
        ALL_DATA <- map(YEARS, .progress=TRUE, possibly(function(yy){
            fname_inner <- file.path("data", "mp02", glue("{yy}_qcew_annual_singlefile.zip"))
            
            if(!file.exists(fname_inner)){
                request("https://www.bls.gov") |> 
                    req_url_path("cew", "data", "files", yy, "csv",
                                 glue("{yy}_annual_singlefile.zip")) |>
                    req_headers(`User-Agent` = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0") |> 
                    req_retry(max_tries=5) |>
                    req_perform(fname_inner)
            }
            
            if(file.info(fname_inner)$size < 755e5){
                warning(sQuote(fname_inner), "appears corrupted. Please delete and retry this step.")
            }
            
            read_csv(fname_inner, 
                     show_col_types=FALSE) |> 
                mutate(YEAR = yy) |>
                select(area_fips, 
                       industry_code, 
                       annual_avg_emplvl, 
                       total_annual_wages, 
                       YEAR) |>
                filter(nchar(industry_code) <= 5, 
                       str_starts(area_fips, "C")) |>
                filter(str_detect(industry_code, "-", negate=TRUE)) |>
                mutate(FIPS = area_fips, 
                       INDUSTRY = as.integer(industry_code), 
                       EMPLOYMENT = as.integer(annual_avg_emplvl), 
                       TOTAL_WAGES = total_annual_wages) |>
                select(-area_fips, 
                       -industry_code, 
                       -annual_avg_emplvl, 
                       -total_annual_wages) |>
                # 10 is a special value: "all industries" , so omit
                filter(INDUSTRY != 10) |> 
                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)
        })) |> bind_rows()
        
        write_csv(ALL_DATA, fname)
    }
    
    ALL_DATA <- read_csv(fname, show_col_types=FALSE)
    
    ALL_DATA_YEARS <- unique(ALL_DATA$YEAR)
    
    YEARS_DIFF <- setdiff(YEARS, ALL_DATA_YEARS)
    
    if(length(YEARS_DIFF) > 0){
        stop("Download failed for the following years: ", YEARS_DIFF, 
             ". Please delete intermediate files and try again.")
    }
    
    ALL_DATA
}

WAGES <- get_bls_qcew_annual_averages()
```

## Data Integration Structure

This analysis integrates data from three federal agencies—the Census Bureau's American Community Survey (ACS), the Census Building Permits database, and the Bureau of Labor Statistics (BLS)—each employing distinct identification systems for Core-Based Statistical Areas (CBSAs). 

The primary analytical challenge lies in reconciling three incompatible geographic identification formats across these authoritative data sources.

```{r}

#| label: fig-relationship-diagram
#| fig-cap: "Data Integration Architecture: Entity Relationships & Transformation Logic Across Federal Data Sources"
#| out-width: "100%"
#| results: 'asis'

library(DiagrammeR)
grViz("
digraph MP02_ERD {
  
  graph [rankdir=TB, fontname=Arial, bgcolor=white, splines=polyline, nodesep=2, ranksep=3.5, margin='1.5,0.5',
         label='Data Integration Architecture:\\nEntity Relationships & Transformation Logic Across Federal Data Sources', 
         labelloc='t', 
         fontsize=50]
  node [shape=box, style='rounded,filled', fontname=Arial, fontsize=28, margin=0.7, width=7, height=2.5]
  edge [fontname=Arial, fontsize=22, labelfloat=true, labeldistance=4]
  
  INCOME [label='INCOME\\n\\nGEOID (PK)\\nNAME\\nyear (PK)\\n\\nhousehold_income', fillcolor='lightblue']
  RENT [label='RENT\\n\\nGEOID (PK)\\nNAME\\nyear (PK)\\n\\nmonthly_rent', fillcolor='lightblue']
  POPULATION [label='POPULATION\\n\\nGEOID (PK)\\nNAME\\nyear (PK)\\n\\npopulation', fillcolor='lightblue']
  HOUSEHOLDS [label='HOUSEHOLDS\\n\\nGEOID (PK)\\nNAME\\nyear (PK)\\n\\nhouseholds', fillcolor='lightblue']
  PERMITS [label='PERMITS\\n\\nCBSA (PK)\\nyear (PK)\\n\\nnew_housing_units_permitted', fillcolor='#90EE90', width=8]
  WAGES [label='WAGES\\n\\nFIPS (PK)\\nINDUSTRY (PK, FK)\\nYEAR (PK)\\n\\nEMPLOYMENT\\nTOTAL_WAGES\\nAVG_WAGE', fillcolor='lightcoral', width=8, height=3]
  INDUSTRY [label='INDUSTRY_CODES\\n\\nlevel4_code (PK)\\n\\nlevel4_title\\nlevel3_code/title\\nlevel2_code/title\\nlevel1_code/title', fillcolor='lightcoral', width=8, height=3]
  
  INCOME -> RENT [label='      GEOID + year      ', color=blue, penwidth=5, fontcolor=blue, fontsize=22]
  RENT -> POPULATION [label='      GEOID + year      ', color=blue, penwidth=5, fontcolor=blue, fontsize=22]
  POPULATION -> HOUSEHOLDS [label='      GEOID + year      ', color=blue, penwidth=5, fontcolor=blue, fontsize=22]
  POPULATION -> PERMITS [label='      Extract CBSA      \\n      from GEOID      ', color=orange, penwidth=5, style=dashed, fontcolor='#CC5500', fontsize=22]
  PERMITS -> WAGES [label='      Format CBSA      \\n      to FIPS      ', color=orange, penwidth=5, style=dashed, fontcolor='#CC5500', fontsize=22]
  WAGES -> INDUSTRY [label='      INDUSTRY to      \\n      level4_code      ', color=red, penwidth=5, fontcolor=red, fontsize=22]
  
  {rank=same; HOUSEHOLDS; PERMITS}
}
", height = "1200px", width = "100%")

```

### Geographic Identification Challenges

As illustrated in Figure 1, each data source employs a different convention for identifying the same geographic units:

- **Census ACS Tables** use alphanumeric `GEOID` strings (e.g., "CBSA35620" for New York City)
- **Census Building Permits** use integer `CBSA` codes (e.g., 35620)
- **BLS Wage Data** use formatted `FIPS` strings with "C" prefix and zero-padding (e.g., "C35620")

These seemingly minor differences represent significant data engineering challenges. A naive join would fail to match any records across these sources, despite all three referring to identical geographic areas.

### Data Integration Methodology

The four ACS tables (INCOME, RENT, POPULATION, HOUSEHOLDS) share identical composite primary keys consisting of GEOID and year, enabling straightforward joins using standard `left_join()` operations. These tables form the analytical foundation, providing essential household economic and demographic metrics at the CBSA-year level.

Connecting ACS data to the Building Permits database requires the first transformation: extracting the numeric CBSA code from the GEOID string. This is accomplished using:
```r
mutate(CBSA = as.integer(str_extract(GEOID, "[0-9]+")))
```

The second transformation bridges Building Permits to BLS wage data by formatting the integer CBSA code into the BLS-specific FIPS string format:
```r
mutate(FIPS = paste0("C", str_pad(CBSA, 5, pad = "0")))
```

Additionally, an important naming convention difference exists: ACS and Census tables use lowercase `year`, while BLS tables use uppercase `YEAR`. Join operations must explicitly map these columns:
```r
left_join(WAGES, by = c("FIPS", "year" = "YEAR"))
```

Finally, the INDUSTRY_CODES lookup table provides hierarchical NAICS classifications that enable occupational analysis. This requires converting the integer INDUSTRY column to character type for matching against level4_code.

### Implications for Analysis Reproducibility

Understanding these transformation requirements is essential for three reasons. First, they ensure **geographic accuracy**—incorrect joining would produce meaningless results or silent errors where CBSAs fail to match. Second, they enable **reproducibility**—future analysts can replicate this work by following the documented transformation logic. Third, they demonstrate **data quality awareness**—recognizing that real-world data integration often requires careful attention to formatting conventions across institutional boundaries.

The relationship diagram serves as both documentation and validation, providing visual confirmation that the analysis correctly handles the complexity of multi-agency federal data integration.

### Question 1: Largest Housing Permits (2010-2019)

Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?

```{r}
# Step 1: Filter PERMITS for 2010-2019 and sum by CBSA
permits_2010_2019 <- PERMITS |>
  filter(year >= 2010 & year <= 2019) |>
  group_by(CBSA) |>
  summarize(total_new_units = sum(new_housing_units_permitted, na.rm = TRUE)) |>
  arrange(desc(total_new_units))

# Step 2: Get CBSA names
cbsa_names <- POPULATION |>
  filter(year == 2019) |>
  mutate(CBSA = as.integer(str_extract(GEOID, "[0-9]+"))) |>
  select(CBSA, NAME)

# Step 3: Join and get top 5
top_5_permits <- permits_2010_2019 |>
  left_join(cbsa_names, by = "CBSA") |>
  head(5) |>
  mutate(rank = row_number()) |>
  select(rank, NAME, total_new_units)

# Get the winner
winner_name <- top_5_permits |> slice(1) |> pull(NAME)
winner_units <- top_5_permits |> slice(1) |> pull(total_new_units)

# Create styled gt table
top_5_permits |>
  gt() |>
  tab_header(
    title = md("**Top 5 CBSAs for New Housing Units Permitted**"),
    subtitle = "2010–2019 (inclusive)"
  ) |>
  cols_label(
    rank = "Rank",
    NAME = "CBSA",
    total_new_units = "Total New Units"
  ) |>
  fmt_number(
    columns = total_new_units,
    decimals = 0
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#d4edda"),
      cell_text(weight = "bold")
    ),
    locations = cells_body(rows = rank == 1)
  ) |>
  tab_source_note(md("Source: **U.S. Census Bureau Building Permits Survey**"))
```

**Answer:** The **`r winner_name`** permitted the largest number of new housing units from 2010 to 2019, with **`r format(winner_units, big.mark = ",")`** total new units permitted during the decade.

### Question 2: Albuquerque Peak Year

In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?

```{r}

# Filter for Albuquerque and get all years
albuquerque_permits <- PERMITS |>
  filter(CBSA == 10740) |>
  arrange(year) |>
  select(year, new_housing_units_permitted)

# Get peak year
peak_year <- albuquerque_permits |>
  slice_max(new_housing_units_permitted, n = 1) |>
  pull(year)

peak_units <- albuquerque_permits |>
  slice_max(new_housing_units_permitted, n = 1) |>
  pull(new_housing_units_permitted)

# Create visualization
ggplot(albuquerque_permits, aes(x = year, y = new_housing_units_permitted)) +
  geom_line(color = "steelblue", size = 1.5) +
  geom_point(size = 3, color = "steelblue") +
  geom_point(
    data = albuquerque_permits |> filter(year == 2020),
    aes(x = year, y = new_housing_units_permitted),
    color = "red", size = 4
  ) +
  geom_point(
    data = albuquerque_permits |> filter(year == 2021),
    aes(x = year, y = new_housing_units_permitted),
    color = "darkgreen", size = 4
  ) +
  annotate("text", x = 2020,
           y = albuquerque_permits$new_housing_units_permitted[albuquerque_permits$year == 2020],
           label = "COVID Dip", color = "red", hjust = 1.2, vjust = 1.5) +
  annotate("text", x = 2021,
           y = albuquerque_permits$new_housing_units_permitted[albuquerque_permits$year == 2021],
           label = "Peak", color = "darkgreen", hjust = 0.5, vjust = -1.5) +
  labs(
    title = "Housing Permits in Albuquerque, NM (CBSA 10740)",
    subtitle = "Note the 2020 COVID dip and 2021 rebound",
    x = "Year",
    y = "New Housing Units Permitted"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma, expand = expansion(mult = c(0.05, 0.15))) +
  coord_cartesian(clip = "off")

# Create styled table showing all years
albuquerque_permits |>
  mutate(year = as.character(year)) |>
  gt() |>
  tab_header(
    title = md("**Albuquerque Housing Permits by Year**"),
    subtitle = "CBSA 10740 (2009–2023)"
  ) |>
  cols_label(
    year = "Year",
    new_housing_units_permitted = "New Housing Units Permitted"
  ) |>
  fmt_number(
    columns = new_housing_units_permitted,
    decimals = 0
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#d1ecf1"),
      cell_text(weight = "bold")
    ),
    locations = cells_body(rows = year == as.character(peak_year))
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#f8d7da"),
      cell_text(style = "italic")
    ),
    locations = cells_body(rows = year == "2020")
  ) |>
  tab_source_note(md("Source: **U.S. Census Bureau Building Permits Survey**"))
```

**Answer:** Albuquerque, NM (CBSA 10740) permitted the most new housing units in **`r peak_year`** with **`r format(peak_units, big.mark = ",")`** units.

**COVID-19 Artifact Note:** The 2021 peak represents a "catch-up" effect following severe disruption to construction permitting in 2020 (highlighted in red in the table) due to COVID-19 lockdowns. The table shows 2020 had the lowest permits in the dataset, followed by a dramatic rebound in 2021 as delayed projects moved forward and pent-up housing demand materialized. This pattern is observed across many metropolitan areas and should be interpreted as COVID-related disruption rather than a structural shift in housing development.


### Question 3: Highest Average Individual Income by State (2015)

Which state (not CBSA) had the highest average individual income in 2015?

```{r}

# Step 1: Join the three tables for 2015
income_2015 <- INCOME |>
  filter(year == 2015) |>
  left_join(HOUSEHOLDS |> filter(year == 2015), by = c("GEOID", "NAME", "year")) |>
  left_join(POPULATION |> filter(year == 2015), by = c("GEOID", "NAME", "year"))

# Step 2: Calculate total income per CBSA and extract state
cbsa_income <- income_2015 |>
  mutate(
    total_income = household_income * households,
    state = str_extract(NAME, ", (.{2})", group = 1)
  ) |>
  select(GEOID, NAME, state, total_income, population)

# Step 3: Sum by state
state_income <- cbsa_income |>
  group_by(state) |>
  summarize(
    total_state_income = sum(total_income, na.rm = TRUE),
    total_state_population = sum(population, na.rm = TRUE)
  ) |>
  mutate(avg_individual_income = total_state_income / total_state_population) |>
  arrange(desc(avg_individual_income))

# Step 4: Get the top state
top_state_abb <- state_income |>
  slice(1) |>
  pull(state)

top_state_income <- state_income |>
  slice(1) |>
  pull(avg_individual_income)

# Step 5: Convert abbreviation to full state name
state_df <- data.frame(
  abb  = c(state.abb, "DC", "PR"),
  name = c(state.name, "District of Columbia", "Puerto Rico")
)

top_state_name <- state_df |>
  filter(abb == top_state_abb) |>
  pull(name)

# Show top 10 states with formatted income
top_10_states <- state_income |>
  head(10) |>
  left_join(state_df, by = c("state" = "abb")) |>
  mutate(
    # Format income with dollar sign and commas
    income_formatted = paste0("$", format(round(avg_individual_income, 2), 
                                         big.mark = ",", 
                                         nsmall = 2)),
    note = case_when(
      state == "DC" ~ "⚠️ 100% Multi-State",
      state %in% c("NY", "MD", "VA", "PA", "NJ") ~ "Contains Multi-State CBSAs",
      TRUE ~ ""
    )
  ) |>
  select(name, state, income_formatted, total_state_population, note)

knitr::kable(top_10_states,
             col.names = c("State", "Abbr", "Avg Individual Income", 
                          "Total Population", "Note"),
             align = "lllll",  # LEFT-ALIGN ALL 5 COLUMNS
             format.args = list(big.mark = ","),
             caption = "Top 10 States by Average Individual Income (2015)")
```

**Answer:** In 2015, the **`r top_state_name`** had the highest average individual income at **$`r format(round(top_state_income, 2), big.mark = ",")`** per person.

**Critical Methodological Limitations:**

1. **Multi-State CBSA Bias:** DC's result is derived entirely from the Washington-Arlington-Alexandria metropolitan area, which spans DC, Virginia, Maryland, and West Virginia. This methodology attributes income from wealthy Virginia and Maryland suburbs to DC, inflating DC's apparent income level. DC represents 100% of a multi-state metro area rather than a standalone state economy.

2. **CBSA Coverage Limitation:** This analysis only includes Core-Based Statistical Areas (metropolitan areas) and excludes rural populations. States with large rural populations are systematically undercounted. This creates an urban-rural bias where highly urbanized states appear to have higher incomes, partially because rural (typically lower-income) populations are excluded from the analysis.

3. **Most Accurate Interpretation:** Among states with primarily single-state CBSAs and high metro coverage, **Massachusetts** and **Connecticut** represent the most reliable results, as they include most of their state populations and don't have the multi-state attribution problem affecting DC.

**Conclusion:** While DC technically ranks first in this analysis, the result reflects methodological artifacts rather than genuine state-level economic comparison. Multi-state CBSAs represent 27.3% of the total population in this dataset. For a fair state-to-state comparison, Massachusetts or Connecticut would be more defensible answers.


### Question 4: Data Scientists by CBSA Over Time

What is the last year in which the NYC CBSA had the most data scientists in the country? In recent, the San Francisco CBSA has had the most data scientists.

```{r}

# Prepare data
data_scientists <- WAGES |>
  filter(INDUSTRY == 51821) |>
  rename(std_cbsa = FIPS, year = YEAR)

cbsa_names <- POPULATION |>
  filter(year == 2019) |>
  mutate(
    CBSA = as.integer(str_extract(GEOID, "[0-9]+")),
    std_cbsa = paste0("C", str_sub(as.character(CBSA), 1, 4))
  ) |>
  select(std_cbsa, NAME)

# Calculate last NYC year properly
last_nyc_year <- data_scientists |>
  group_by(std_cbsa, year) |>
  summarize(total_employment = sum(EMPLOYMENT, na.rm = TRUE), .groups = "drop") |>
  left_join(cbsa_names, by = "std_cbsa") |>
  group_by(year) |>
  slice_max(total_employment, n = 1, with_ties = FALSE) |>
  ungroup() |>
  filter(str_detect(NAME, "New York")) |>
  pull(year) |>
  max()

# Focus on the transition period: 2014-2017
transition_years <- data_scientists |>
  filter(year %in% c(2014, 2015, 2016, 2017)) |>
  group_by(std_cbsa, year) |>
  summarize(total_employment = sum(EMPLOYMENT, na.rm = TRUE), .groups = "drop") |>
  left_join(cbsa_names, by = "std_cbsa") |>
  group_by(year) |>
  slice_max(total_employment, n = 3, with_ties = FALSE) |>
  ungroup() |>
  arrange(year, desc(total_employment)) |>
  group_by(year) |>
  mutate(rank = row_number()) |>
  ungroup() |>
  mutate(year = as.character(year)) |>
  select(year, rank, NAME, total_employment)

knitr::kable(transition_years,
             col.names = c("Year", "Rank", "CBSA", "Employment"),
             align = "llll",
             format.args = list(big.mark = ","),
             caption = "The NYC-to-San Francisco Transition (2014-2017)")

```

**Answer:** The last year in which the New York City CBSA had the most data scientists and business analysts (NAICS 51821) in the country was **`r last_nyc_year`**. As shown in the table above, San Francisco has led the nation since then, reflecting the tech industry's concentration in Silicon Valley.

### Question 5: Finance & Insurance Wages in NYC

What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?

```{r}

# Identify NYC CBSA code
nyc_cbsa_code <- "C3562"

# Calculate finance wages as fraction of total wages by year
nyc_wage_share <- WAGES |>
  filter(FIPS == nyc_cbsa_code, YEAR != 2020) |>  # Omit 2020 per data issues
  mutate(
    is_finance = str_starts(as.character(INDUSTRY), "52")
  ) |>
  group_by(year = YEAR) |>
  summarize(
    wages_total = sum(TOTAL_WAGES, na.rm = TRUE),
    wages_fin = sum(if_else(is_finance, TOTAL_WAGES, 0), na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    share_fin = wages_fin / wages_total
  ) |>
  arrange(year)

# Get peak year
peak_year <- nyc_wage_share$year[which.max(nyc_wage_share$share_fin)]
peak_share <- max(nyc_wage_share$share_fin) * 100

# Create professional gt table
nyc_wage_share |>
  select(
    Year = year,
    `Total Wages (USD)` = wages_total,
    `Finance & Insurance Wages (USD)` = wages_fin,
    `Finance Share` = share_fin
  ) |>
  gt() |>
  tab_header(
    title = md("**NYC CBSA — Finance & Insurance Wage Share**"),
    subtitle = "BLS QCEW 2009–2023 • NAICS 52 • 2020 omitted"
  ) |>
  fmt_currency(
    columns = c(`Total Wages (USD)`, `Finance & Insurance Wages (USD)`),
    decimals = 0
  ) |>
  fmt_percent(columns = `Finance Share`, decimals = 1) |>
  cols_label(`Finance Share` = "Finance Share of Total") |>
  tab_style(
    style = list(
      cell_fill(color = "#fff3cd"),
      cell_text(weight = "bold")
    ),
    locations = cells_body(rows = Year == peak_year)
  ) |>
  tab_source_note(md("Source: **BLS QCEW**; NAICS 52."))

```

**Answer:** 

**Part A - The Fraction:** Finance and insurance workers earned between **9.4%** and **15.9%** of total wages in the NYC CBSA across the study period.

**Part B - Peak Year:** This fraction **peaked in `r peak_year` at `r round(peak_share, 1)`%**, as highlighted in the table above. The 2021 peak represents a recovery from the declining share observed through the mid-2010s, exceeding even the 2010 post-financial-crisis level (15.4%)

**Context:** The prominence of finance and insurance in NYC's economy reflects the city's role as a global financial capital, though the fraction has fluctuated over time due to economic cycles and industry shifts.
